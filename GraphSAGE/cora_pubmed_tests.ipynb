{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grabias.m\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSAGE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple supervised GraphSAGE model as well as examples running the model\n",
    "on the Cora and Pubmed datasets.\n",
    "\"\"\"\n",
    "\n",
    "class MeanAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Aggregates a node's embeddings using mean of neighbors' embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, features, cuda=False, gcn=False): \n",
    "        \"\"\"\n",
    "        Initializes the aggregator for a specific graph.\n",
    "        features -- function mapping LongTensor of node ids to FloatTensor of feature values.\n",
    "        cuda -- whether to use GPU\n",
    "        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style\n",
    "        \"\"\"\n",
    "\n",
    "        super(MeanAggregator, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.cuda = cuda\n",
    "        self.gcn = gcn\n",
    "        \n",
    "    def forward(self, nodes, to_neighs, num_sample=10):\n",
    "        \"\"\"\n",
    "        nodes --- list of nodes in a batch\n",
    "        to_neighs --- list of sets, each set is the set of neighbors for node in batch\n",
    "        num_sample --- number of neighbors to sample. No sampling if None.\n",
    "        \"\"\"\n",
    "        # Local pointers to functions (speed hack)\n",
    "        _set = set\n",
    "        if not num_sample is None:\n",
    "            _sample = random.sample\n",
    "            samp_neighs = [_set(_sample(to_neigh, \n",
    "                            num_sample,\n",
    "                            )) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
    "        else:\n",
    "            samp_neighs = to_neighs\n",
    "\n",
    "        if self.gcn:\n",
    "            samp_neighs = [samp_neigh + set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n",
    "        unique_nodes_list = list(set.union(*samp_neighs))\n",
    "        unique_nodes = {n:i for i,n in enumerate(unique_nodes_list)}\n",
    "        mask = torch.zeros(len(samp_neighs), len(unique_nodes))\n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]   \n",
    "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "        mask[row_indices, column_indices] = 1\n",
    "        if self.cuda:\n",
    "            mask = mask.cuda()\n",
    "        num_neigh = mask.sum(1, keepdim=True)\n",
    "        mask = mask.div(num_neigh)\n",
    "        if self.cuda:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "        else:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "        to_feats = mask.mm(embed_matrix)\n",
    "        return to_feats\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes a node's using 'convolutional' GraphSage approach\n",
    "    \"\"\"\n",
    "    def __init__(self, features, feature_dim, \n",
    "            embed_dim, adj_lists, aggregator,\n",
    "            num_sample=10,\n",
    "            base_model=None, gcn=False, cuda=False, \n",
    "            feature_transform=False): \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.feat_dim = feature_dim\n",
    "        self.adj_lists = adj_lists\n",
    "        self.aggregator = aggregator\n",
    "        self.num_sample = num_sample\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "\n",
    "        self.gcn = gcn\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "        self.weight = nn.Parameter(\n",
    "                torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2 * self.feat_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a batch of nodes.\n",
    "        nodes     -- list of nodes\n",
    "        \"\"\"\n",
    "        neigh_feats = self.aggregator.forward(nodes,\n",
    "                    [self.adj_lists[int(node)] for node in nodes], self.num_sample)\n",
    "        if not self.gcn:\n",
    "            if self.cuda:\n",
    "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "            else:\n",
    "                self_feats = self.features(torch.LongTensor(nodes))\n",
    "            combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "        else:\n",
    "            combined = neigh_feats\n",
    "        combined = F.relu(self.weight.mm(combined.t()))\n",
    "        return combined\n",
    "\n",
    "\n",
    "class SupervisedGraphSage(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        embeds = self.enc(nodes)\n",
    "        scores = self.weight.mm(embeds)\n",
    "        return scores.t()\n",
    "\n",
    "    def loss(self, nodes, labels):\n",
    "        scores = self.forward(nodes)\n",
    "        return self.xent(scores, labels.squeeze())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cora():\n",
    "    num_nodes = 2708\n",
    "    num_feats = 1433\n",
    "    feat_data = np.zeros((num_nodes, num_feats))\n",
    "    labels = np.empty((num_nodes,1), dtype=np.int64)\n",
    "    node_map = {}\n",
    "    label_map = {}\n",
    "    with open(\"./cora/cora.content\") as fp:\n",
    "        for i,line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            feat_data[i,:] = [float(x) for x in info[1:-1]]\n",
    "            node_map[info[0]] = i\n",
    "            if not info[-1] in label_map:\n",
    "                label_map[info[-1]] = len(label_map)\n",
    "            labels[i] = label_map[info[-1]]\n",
    "\n",
    "    adj_lists = defaultdict(set)\n",
    "    with open(\"./cora/cora.cites\") as fp:\n",
    "        for i,line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            paper1 = node_map[info[0]]\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "    return feat_data, labels, adj_lists\n",
    "\n",
    "def load_pubmed():\n",
    "    #hardcoded for simplicity...\n",
    "    num_nodes = 19717\n",
    "    num_feats = 500\n",
    "    feat_data = np.zeros((num_nodes, num_feats))\n",
    "    labels = np.empty((num_nodes, 1), dtype=np.int64)\n",
    "    node_map = {}\n",
    "    with open(\"pubmed/Pubmed-Diabetes.NODE.paper.tab\") as fp:\n",
    "        fp.readline()\n",
    "        feat_map = {entry.split(\":\")[1]:i-1 for i,entry in enumerate(fp.readline().split(\"\\t\"))}\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.split(\"\\t\")\n",
    "            node_map[info[0]] = i\n",
    "            labels[i] = int(info[1].split(\"=\")[1])-1\n",
    "            for word_info in info[2:-1]:\n",
    "                word_info = word_info.split(\"=\")\n",
    "                feat_data[i][feat_map[word_info[0]]] = float(word_info[1])\n",
    "    adj_lists = defaultdict(set)\n",
    "    with open(\"pubmed/Pubmed-Diabetes.DIRECTED.cites.tab\") as fp:\n",
    "        fp.readline()\n",
    "        fp.readline()\n",
    "        for line in fp:\n",
    "            info = line.strip().split(\"\\t\")\n",
    "            paper1 = node_map[info[1].split(\":\")[1]]\n",
    "            paper2 = node_map[info[-1].split(\":\")[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "    return feat_data, labels, adj_lists\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cora():\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    \n",
    "    num_nodes = 2708\n",
    "    feat_data, labels, adj_lists = load_cora()\n",
    "    \n",
    "    features = nn.Embedding(2708, 1433)\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data),\n",
    "                                   requires_grad=False)\n",
    "\n",
    "    agg1 = MeanAggregator(features, cuda=True)\n",
    "    \n",
    "    enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True,\n",
    "                          cuda=False)\n",
    "    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(),\n",
    "                          cuda=False)\n",
    "    enc2 = Encoder(lambda nodes : enc1(nodes).t(),\n",
    "                   enc1.embed_dim, 128, adj_lists, agg2, \n",
    "                   base_model=enc1, gcn=True, cuda=False)\n",
    "    \n",
    "    enc1.num_samples = 5\n",
    "    enc2.num_samples = 5\n",
    "\n",
    "    graphsage = SupervisedGraphSage(7, enc2)\n",
    "    rand_indices = np.random.permutation(num_nodes)\n",
    "    test = rand_indices[:1000]\n",
    "    val = rand_indices[1000:1500]\n",
    "    train = list(rand_indices[1500:])\n",
    "\n",
    "    optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad,\n",
    "                                       graphsage.parameters()), lr=0.7)\n",
    "    times = []\n",
    "    \n",
    "    for batch in range(100):\n",
    "        batch_nodes = train[:256]\n",
    "        random.shuffle(train)\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = graphsage.loss(batch_nodes, \n",
    "                torch.LongTensor(labels[np.array(batch_nodes)]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "        print (batch, loss.item())\n",
    "\n",
    "    val_output = graphsage.forward(val)\n",
    "    \n",
    "    print (\"Validation F1:\", f1_score(labels[val],\n",
    "                            val_output.data.numpy().argmax(axis=1),\n",
    "                            average=\"micro\"))\n",
    "    \n",
    "    print (\"Average batch time:\", np.mean(times))\n",
    "\n",
    "def run_pubmed():\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    num_nodes = 19717\n",
    "    feat_data, labels, adj_lists = load_pubmed()\n",
    "    features = nn.Embedding(19717, 500)\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "   # features.cuda()\n",
    "\n",
    "    agg1 = MeanAggregator(features, cuda=True)\n",
    "    enc1 = Encoder(features, 500, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "    enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,\n",
    "            base_model=enc1, gcn=True, cuda=False)\n",
    "    enc1.num_samples = 10\n",
    "    enc2.num_samples = 25\n",
    "\n",
    "    graphsage = SupervisedGraphSage(3, enc2)\n",
    "#    graphsage.cuda()\n",
    "    rand_indices = np.random.permutation(num_nodes)\n",
    "    test = rand_indices[:1000]\n",
    "    val = rand_indices[1000:1500]\n",
    "    train = list(rand_indices[1500:])\n",
    "\n",
    "    optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)\n",
    "    times = []\n",
    "    for batch in range(500):\n",
    "        batch_nodes = train[:1024]\n",
    "        random.shuffle(train)\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = graphsage.loss(batch_nodes, torch.LongTensor(labels[np.array(batch_nodes)]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "        print(batch, loss.item())\n",
    "\n",
    "    val_output = graphsage.forward(val) \n",
    "    print(\"Validation F1:\", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "    print(\"Average batch time:\", np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (914038240.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def get_encodings(model):\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def get_encodings(model):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "    encodings = \n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grabias.m\\AppData\\Local\\Temp\\ipykernel_10112\\2996974090.py:84: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)\n",
      "C:\\Users\\grabias.m\\AppData\\Local\\Temp\\ipykernel_10112\\2996974090.py:113: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)\n",
      "C:\\Users\\grabias.m\\AppData\\Local\\Temp\\ipykernel_10112\\2996974090.py:34: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  samp_neighs = [_set(_sample(to_neigh,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1009242534637451\n",
      "1 1.099691390991211\n",
      "2 1.0999482870101929\n",
      "3 1.0992933511734009\n",
      "4 1.0988823175430298\n",
      "5 1.098703145980835\n",
      "6 1.0982341766357422\n",
      "7 1.0979974269866943\n",
      "8 1.097416877746582\n",
      "9 1.0972274541854858\n",
      "10 1.0968600511550903\n",
      "11 1.096621036529541\n",
      "12 1.0960773229599\n",
      "13 1.0960546731948853\n",
      "14 1.095515251159668\n",
      "15 1.0952186584472656\n",
      "16 1.094924807548523\n",
      "17 1.094451904296875\n",
      "18 1.094401478767395\n",
      "19 1.093504548072815\n",
      "20 1.0937119722366333\n",
      "21 1.0929173231124878\n",
      "22 1.0927454233169556\n",
      "23 1.092276692390442\n",
      "24 1.0924618244171143\n",
      "25 1.0915111303329468\n",
      "26 1.0909135341644287\n",
      "27 1.0912710428237915\n",
      "28 1.0903468132019043\n",
      "29 1.089837670326233\n",
      "30 1.0884822607040405\n",
      "31 1.089142918586731\n",
      "32 1.0887489318847656\n",
      "33 1.0879244804382324\n",
      "34 1.086766004562378\n",
      "35 1.0852636098861694\n",
      "36 1.0869019031524658\n",
      "37 1.0851194858551025\n",
      "38 1.0848233699798584\n",
      "39 1.08475923538208\n",
      "40 1.0830601453781128\n",
      "41 1.0818195343017578\n",
      "42 1.0827137231826782\n",
      "43 1.0814510583877563\n",
      "44 1.0803959369659424\n",
      "45 1.079126238822937\n",
      "46 1.0787163972854614\n",
      "47 1.0793462991714478\n",
      "48 1.0769752264022827\n",
      "49 1.0778937339782715\n",
      "50 1.0751363039016724\n",
      "51 1.074820876121521\n",
      "52 1.0733001232147217\n",
      "53 1.0728315114974976\n",
      "54 1.0702860355377197\n",
      "55 1.0705783367156982\n",
      "56 1.0683518648147583\n",
      "57 1.070533275604248\n",
      "58 1.0674142837524414\n",
      "59 1.0637478828430176\n",
      "60 1.063170313835144\n",
      "61 1.0633022785186768\n",
      "62 1.0578068494796753\n",
      "63 1.05894136428833\n",
      "64 1.053406834602356\n",
      "65 1.0585997104644775\n",
      "66 1.0511282682418823\n",
      "67 1.0512200593948364\n",
      "68 1.0447851419448853\n",
      "69 1.044607400894165\n",
      "70 1.0424036979675293\n",
      "71 1.0427964925765991\n",
      "72 1.0387626886367798\n",
      "73 1.0421183109283447\n",
      "74 1.0341129302978516\n",
      "75 1.0237212181091309\n",
      "76 1.0259089469909668\n",
      "77 1.0274957418441772\n",
      "78 1.0246894359588623\n",
      "79 1.0196409225463867\n",
      "80 1.014711856842041\n",
      "81 1.015043020248413\n",
      "82 1.012636423110962\n",
      "83 1.0110946893692017\n",
      "84 1.0008152723312378\n",
      "85 1.0035523176193237\n",
      "86 1.0009891986846924\n",
      "87 0.9979555606842041\n",
      "88 0.9750664830207825\n",
      "89 0.9798988103866577\n",
      "90 0.9758732318878174\n",
      "91 0.9828648567199707\n",
      "92 0.9663097262382507\n",
      "93 0.9664613604545593\n",
      "94 0.9657337069511414\n",
      "95 0.9534872174263\n",
      "96 0.9489642977714539\n",
      "97 0.9506403207778931\n",
      "98 0.9453526139259338\n",
      "99 0.9353621006011963\n",
      "100 0.9352211356163025\n",
      "101 0.9253091216087341\n",
      "102 0.9202374815940857\n",
      "103 0.9215517044067383\n",
      "104 0.9103373289108276\n",
      "105 0.9026497602462769\n",
      "106 0.9073249101638794\n",
      "107 0.8984382748603821\n",
      "108 0.8947861194610596\n",
      "109 0.8844994902610779\n",
      "110 0.8646885752677917\n",
      "111 0.8802144527435303\n",
      "112 0.8641820549964905\n",
      "113 0.8618727326393127\n",
      "114 0.8454102873802185\n",
      "115 0.8415033221244812\n",
      "116 0.8334481716156006\n",
      "117 0.8386655449867249\n",
      "118 0.8298577666282654\n",
      "119 0.82613205909729\n",
      "120 0.836357593536377\n",
      "121 0.7970653772354126\n",
      "122 0.8036709427833557\n",
      "123 0.7953709363937378\n",
      "124 0.7979356050491333\n",
      "125 0.7842544317245483\n",
      "126 0.7883726358413696\n",
      "127 0.7871595621109009\n",
      "128 0.7607290148735046\n",
      "129 0.7461832165718079\n",
      "130 0.745979368686676\n",
      "131 0.7404914498329163\n",
      "132 0.7336381077766418\n",
      "133 0.7245181202888489\n",
      "134 0.7145963907241821\n",
      "135 0.7182678580284119\n",
      "136 0.7190317511558533\n",
      "137 0.7173541188240051\n",
      "138 0.7137044668197632\n",
      "139 0.6923581957817078\n",
      "140 0.6997464895248413\n",
      "141 0.6942808628082275\n",
      "142 0.6788778305053711\n",
      "143 0.685420572757721\n",
      "144 0.6483472585678101\n",
      "145 0.6587558388710022\n",
      "146 0.6379149556159973\n",
      "147 0.6437013149261475\n",
      "148 0.6576479077339172\n",
      "149 0.6340691447257996\n",
      "150 0.6373134255409241\n",
      "151 0.6184546947479248\n",
      "152 0.6283000707626343\n",
      "153 0.6284669041633606\n",
      "154 0.5976108908653259\n",
      "155 0.6018909811973572\n",
      "156 0.6114882826805115\n",
      "157 0.5846351981163025\n",
      "158 0.5789439678192139\n",
      "159 0.5719169974327087\n",
      "160 0.5924107432365417\n",
      "161 0.5776163935661316\n",
      "162 0.5620766878128052\n",
      "163 0.5731838345527649\n",
      "164 0.5627927184104919\n",
      "165 0.5233563184738159\n",
      "166 0.5578606128692627\n",
      "167 0.5462451577186584\n",
      "168 0.5259952545166016\n",
      "169 0.503027081489563\n",
      "170 0.5280820727348328\n",
      "171 0.5310065150260925\n",
      "172 0.5569356679916382\n",
      "173 0.534157395362854\n",
      "174 0.5110188126564026\n",
      "175 0.5113492012023926\n",
      "176 0.5397549271583557\n",
      "177 0.5090981721878052\n",
      "178 0.5314253568649292\n",
      "179 0.5166081190109253\n",
      "180 0.5079750418663025\n",
      "181 0.4948503375053406\n",
      "182 0.474065363407135\n",
      "183 0.47203683853149414\n",
      "184 0.47560015320777893\n",
      "185 0.5260168313980103\n",
      "186 0.47215864062309265\n",
      "187 0.4616445302963257\n",
      "188 0.48601245880126953\n",
      "189 0.47170472145080566\n",
      "190 0.4914815425872803\n",
      "191 0.49601587653160095\n",
      "192 0.4621821343898773\n",
      "193 0.46428924798965454\n",
      "194 0.4515022039413452\n",
      "195 0.5132421255111694\n",
      "196 0.47278517484664917\n",
      "197 0.49075600504875183\n",
      "198 0.44989413022994995\n",
      "199 0.44883784651756287\n",
      "Validation F1: 0.81\n",
      "Average batch time: 0.3094677662849426\n"
     ]
    }
   ],
   "source": [
    "run_pubmed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n",
    "- [Graph Node Embedding Algorithms (Stanford - Fall 2019) by Jure Leskovec](https://www.youtube.com/watch?v=7JELX6DiUxQ)\n",
    "- [Jure Leskovec: \"Large-scale Graph Representation Learning\"](https://www.youtube.com/watch?v=oQL4E1gK3VU)\n",
    "- [Jure Leskovec \"Deep Learning on Graphs\"\n",
    "](https://www.youtube.com/watch?v=MIAbDNAxChI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "77845d2f7d5fa83174d5cbd19bb448ec4c04be8e20fe1d69a90ac8ab1cfaea9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
